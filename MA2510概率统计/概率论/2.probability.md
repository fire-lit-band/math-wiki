# definition
## sample space
The set of all possible outcomes of an experiment
Each of the possible outcome in S is called a simple event or a basic  outcome
## Event
Any subset of the sample space. If the outcome of
An event occurs when any one of the outcomes in the event occurs
# relation
Mutually Exclusive 完全不同 
collectively exhaustive 合起来构成一个全部
Intersection
A and B=$A \cap B$
P(A and B )=P($A \cap B$)
Union
A or B =$A \cup B$
P(A or B)=P($A \cup B$)
Complement
Difference:
# probablity
## definition
probability, which is a numerical value, is assigned to each event to denote the chance that the event will occur P(E), the probability of the event E , is defined, as the limiting frequency of E when an experiment is performed repeatedly , $P(E)=\lim \limits_{n \rightarrow \infty}\frac{n(E)}{n}$
### Axiomatic approach to Probability
A probability function (or probability measure) on the events in a sample space is a function on the events that satisfies the following three axioms:
 Axiom 1: For any event E, $0 \leq P(E) \leq 1$
Axiom 2: $P(S)=1$, where $S$ is the sample space 
Axiom 3: For any sequence of mutually exclusive events (that is, events for which $E_{i} E_{i}=\emptyset i \neq j$ ),
$$
P\left(\cup_{i=1}^{\infty} E_{i}\right)=\sum_{i=1}^{\infty} P\left(E_{i}\right)
$$
## properties
Properties of probability function
1. $P(\emptyset)=0$
Proof: Consider the sequence of events,
$$
E_{1}=S, E_{2}=E_{3}=\cdots=\emptyset
$$
Then, as the events are mutually exclusive and $S=\cup_{i=1}^{\infty} E_{i}$, we have from Axiom 3 that
$$
P(\emptyset)=\sum_{i=1}^{\infty} P\left(E_{i}\right)=P(S)+\left(\sum_{i=2}^{\infty} P(\emptyset)\right)=0
$$
implying that $P(\emptyset)=0$
2. (finite additivity) For any finite sequence of mutually exclusive events $E_{1}, \ldots, E_{n}, P\left(\cup_{i=1}^{n} E_{i}\right)=\sum_{i=1}^{n} P\left(E_{i}\right)$
3. P(A or B)=P($A \cup B$)=P(A)+P(B)-P($A \cap B$)
4. 3. $\quad P\left(E^{c}\right)=1-P(E)$
Proof: $\quad P\left({E})+P\left({E}^{c}\right)=P\left(E \cup E^{c}\right)=P(S)=1\right.$
4 If E$\subseteq { F }$, then $P(E) \leq P(F)$
Proof: $\quad P(F)=P(E)+P\left(E^{c} \cap F\right) \geq P(E)$
Proposition Suppose $E$ is independent of $F$. We will now show that $E$ is also independent of $F^{c}$
For $n$ events, we have the multiplication rule:
$$
P\left(E_{1} E_{2} \cdots E_{n}\right)=P\left(E_{1}\right) P\left(E_{2} \mid E_{1}\right) P\left(E_{3} \mid E_{1} E_{2}\right) \cdots P\left(E_{n} \mid E_{1} \cdots E_{n-1}\right)
$$
## method
### priori classical probability method  
Calculate the probability objectively based on prior or  theoretical knowledge of the process  
Assume the outcomes are equally likely to occur.
### Empirical method (relative frequency method)  
Calculate the probability objectively based on observed  data  
Repeat the experiment n times under the same condition.  
The empirical probability of an event is determined by the  
number of times the event occurred (relative frequency)
## conditional probablity
The conditional probability of event $A$ given event $B$ occurs, denoted by $P(A \mid B)$, is defined as
$P(A \mid B)=\frac{P(A \text { and } B)}{P(B)} \quad$ 
 $P(A), P(B)$ are called Marginal probability - probability of only 1 event occurring
P(A and B) is called Joint probability - probability of 2 or more events occurring together
 Multiplication rule
$P(A$ and $B)=P(A \mid B) P(B)=P(B \mid A) P(A)$
## Statistical independence
Two events, $A$ and $B$, are independent if the occurrence of event A does not affect the probability of occurrence of event $B$, or vice versa
 $P(A \mid B)=P(A)$, or$P(B \mid A)=P(B)$
$P(A$ and $B)=P(A) P(B)$
P（S|F）=1
## law of total probability
The Law of Total Probability: Suppose $F_{1}, F_{2}, \ldots, F_{n} \quad$ are mutually exclusive events such that $\cup_{i=1}^{n} F_{i}=S$ Then, for any event $E, P(E)=\sum_{i=1}^{n} P\left(E \mid F_{i}\right) P\left(F_{i}\right)$
One-line proof:
$$
P(E)=P\left(\cup_{i}\left(E F_{i}\right)\right)=\sum_{i} P\left(E F_{i}\right)=\sum_{i} P\left(E \mid F_{i}\right) P\left(F_{i}\right)$$
## booles' inequality
For any sequence of events (Boole's inequality):
$$
P\left(\cup_{i=1}^{\infty} E_{i}\right) \leq \sum_{i=1}^{\infty} P\left(E_{i}\right)
$$
## 贝叶斯定理
Bayes' formula:
Suppose $F_{1}, F_{2}, \ldots, F_{n}$ are mutually exclusive events such that $\cup_{i=1}^{n} F_{i}=S$. Then, for any event $E$,
$$
P\left(F_{j} \mid E\right)=\frac{P\left(E F_{j}\right)}{P(E)}=\frac{P\left(E \mid F_{j}\right) P\left(F_{j}\right)}{\sum_{i=1}^{n} P\left(E \mid F_{i}\right) P\left(F_{i}\right)}
$$
![[Pasted image 20220214104358.png]]


