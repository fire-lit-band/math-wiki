
# Hypergeometric Random Variables
Suppose that a sample of size $n$ is to be chosen randomly (without replacement) from an urn containing $N$ balls, which $K$ are white and $N-K$ are black. Let $X$ denote the number of white balls selected, then
# continuous
A continuous variable is a variable that can be assume any value on a continuum (can assume an uncountable number of values)
$P(a \leq X \leq b)=\int_{a}^{b} f(x) d x$
$F(x)=P(X \leq x)=\int_{-\infty}^{x} f(y) d y \quad \text { for }-\infty<x<\infty$

Definition: $X$ is said to be a continuous random variable if there exists a nonnegative function $\mathrm{f}$, defined for all real $x \in(-\infty, \infty)$, having the property that for any set $B$ of real number, the probability that $X$ will be in a set $B$ is $P\{X \in B\}=\int_{B} f(x) d x$ $f(x)$ is called pdf of $X$.
Since $X$ must take on some value (in this course it is usually assumed that $X$ cannot be infinite ), the pdf must satisfy: $1=P\{X \in(-\infty, \infty)\}=\int_{-\infty}^{\infty} f(x) d x$
Interpretation of the pdf: Note that
$$
P\left\{a-\frac{\epsilon}{2} \leq X \leq a+\frac{\epsilon}{2}\right\}=\int_{a-\epsilon / 2}^{a+\epsilon / 2} f(x) d x \sim \epsilon f(a)
$$
when $\epsilon$ is small and when $f(x)$ is continuous at $x=a$. So the probability that $X$ will be contained in an interval of length $\varepsilon$ around the point $a$ is approximately $\epsilon f(a)$

# probaiblity density function
One may consider a density function is an approximation to the percentage polygon of its relative frequency distribution
# uniform random variables
A random variable is said to be uniformly distributed over the interval $(\alpha, \beta)$ or $[\alpha, \beta]$ if its pdf is given by $f(x)= \begin{cases}\frac{1}{\beta-\alpha} & \text { if } \alpha<x<\beta \\ 0 & \text { otherwise }\end{cases}$
The CDF of a uniform random variable is
$$
F(x)= \begin{cases}0 & x \leq \alpha \\ \frac{x-\alpha}{\beta-\alpha} & \alpha<x<\beta \\ 1 & x \geq \beta\end{cases}
$$
Example: Show that the expectation and variance of a random variable that is uniformly distributed on $(\alpha, \beta)$ is $E[X]=\frac{\beta+\alpha}{2}, \operatorname{Var}(x)=\frac{(\beta-\alpha)^{2}}{12}$
# Geometric Random Variable
$P\{X=n\}=(1-p)^{n-1} p, n=1,2, \ldots$
$E[X]=1 / p, \operatorname{Var}(X)=\frac{1-p}{p^{2}}$
# Negative Binomial Distribution
Suppose that independent trials, each having a probability $p, 0<p<1$, of being a success, are performed until $r$ successes occur. Let $X$ be the random variable that denotes the number of trials required. The probability mass function of $X$ is
$$
P\{X=n\}=\left(\begin{array}{c}
n-1 \\
r-1
\end{array}\right) p^{r}(1-p)^{n-r}, \quad n=r, r+1, \cdots-
$$
Definition: A random variable whose pmf is given by the above is called a negative binomial random variable with parameters $(r, p)$.
$E[X]=r / p, \operatorname{Var}(X)=\frac{r(1-p)}{p^{2}}$
# normal distribution
X is a normal/Gaussian random variable
$f(x)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\left(\frac{1}{2}\right)\left[\frac{x-\mu}{\sigma}\right]^{2}}$
When $\mu=0$ and $\sigma=1$, the normal random variable $X$ is called a standard normal random variable (denoted Z) and the normal distribution is called a standard (standardized) normal distribution. We write $\mathrm{Z} \sim N(0,1)$ where $-\infty<\mathrm{Z}<+\infty$
CDF of a standard normal distribution: $\Phi(x)=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{x} e^{-y^{2} / 2} d y$ One property of $\Phi$ is that $\Phi(-x)=1-\Phi(x)$
That is, the CDF of a normal r.v. with parameters $\mu$ and $\sigma^{2}$ is $\Phi\left(\frac{x-\mu}{\sigma}\right)$
![](Pasted%20image%2020220223203651.png)
## the empirical rule
Area within $\mu \pm \sigma$ equals $68 \%$ approximately
Area within $\mu \pm 2 \sigma$ equals $95 \%$ approximately
Area within $\mu \pm 3 \sigma$ equals $99.7 \%$ approximately

# exponential distribution
Definition: A random variable with density function
$$
f(x)= \begin{cases}\lambda e^{-\lambda x} & \text { if } x \geq 0 \\ 0 & \text { if } x<0\end{cases}
$$
is called an exponential random variable with parameter $\lambda$ The CDF is $F(a)=\int_{0}^{a} \lambda e^{-\lambda x} d x=-\left.e^{-\lambda x}\right|_{0} ^{a}=1-e^{-\lambda a}, a \geq 0$
E(x)=$1/\lambda$,var(X)=$1/\lambda ^2$
Memorylessness of exponential random variable:
Let $X$ be an exponential random variable with parameter $\lambda$ We have for all $s, t \geq 0$,
$$
\begin{aligned}
P\{X>s+t \mid X>t\} &=\frac{P\{X>s+t \cap X>t\}}{P\{X>t\}} \\
&=\frac{P\{X>s+t\}}{P\{X>t\}}=\frac{1-F(s+t)}{1-F(t)} \\
&=e^{-\lambda(t+s)} / e^{-\lambda t}=e^{\lambda s}=P\{X>s\}
\end{aligned}
$$
Remark: the geometric distribution is also memoryless ( in this case $s, t>0$ are integers)
# gamma distribution
Gamma distribution:
Definition: a random variable $X$ is said to be gamma distributed with parameters $(\alpha, \lambda)$, denoted as $X \sim \operatorname{Gamma}(\alpha, \lambda)$ if its pdf is given by
$$
f(x)= \begin{cases}\frac{\lambda e^{-\lambda x}(\lambda x)^{\alpha-1}}{\Gamma(\alpha)} & x \geq 0 \\ 0 & x<0\end{cases}
$$
The gamma function $\Gamma$ is given by $\Gamma(\alpha)=\int_{0}^{\infty} t^{\alpha-1} e^{-t} d t$ One important property of $\Gamma$ is that $\Gamma(\alpha)=(\alpha-1) \Gamma(\alpha-1)$ For integer value of $\alpha=n, \Gamma(n)=(n-1) !$


