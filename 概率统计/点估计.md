实际上点估计这边的方法更像是经验方法，而不是一个完备的公理性做法，否则也不太可能进行那么多次的估计，还要各种七七八八的操作，这么麻烦（估计的方法是不严谨的，是一种感觉，而无偏的时候是验证）

# Method of moments estimator (MME)

General Description
Let $X_{1}, \ldots, X_{n}$ be iid sample from $f\left(x \mid \theta_{1}, \ldots, \theta_{k}\right)$.
Define the moment

$m_{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}$这个又叫原点矩
$$
\begin{array}{rlr}
m_{1}=\frac{1}{n} \sum_{i=1}^{n} X_{i}=E X \\
m_{2}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}=E X^{2} \\
m_{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}=E X^{k}
\end{array}
$$

我们认为算出来的矩和预期的矩（用EX），进而进行估计

# Maximum Likelihood estimator (MLE)

Definition of the likelihood function For a sample $X=\left(X_{1}, \ldots, X_{n}\right)$ with joint pdf or pmf $f(X \mid \theta)$, the likelihood function is just the pdf or pmf, but think of it as a function of $\theta$ :
$$
L(\theta \mid X)=f(X \mid \theta)
$$
In this course, we have always that observations are i.i.d. Therefore, the resulting density for the samples is
$$
L(\theta \mid X)=\prod_{i=1}^{n} f\left(X_{i} \mid \theta\right)
$$
The maximum likelihood estimator (MLE) is just the maximizer $\hat{\theta}(X)$ of the likelihood function. Often we maximize $l(\theta \mid X)=\log L(\theta \mid X)$ instead because it is usually easier.
Invariance Property of MLE
If $\hat{\theta}$ is the MLE of $\theta$, then for any function $h(\theta)$, $h(\hat{\theta})$ is the MLE of $h(\theta) .$
$$
\begin{aligned}
L &=\prod_{i=1}^{n}\left[\left(\sqrt{2 \pi \sigma^{2}}\right)^{-1} \exp \left(-\frac{1}{2 \sigma^{2}}\left(X_{i}-\mu\right)^{2}\right)\right] \\
\log L &=-\frac{n}{2} \log (2 \pi)-\frac{n}{2} \log \left(\sigma^{2}\right)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}
\end{aligned}
$$
求方䅡组 $\left(2.5\right.$ ) (把 $\sigma^{2}$ 作为一个整体看):
$$
\begin{gathered}
\frac{\partial \log L}{\partial \mu}=\frac{1}{\sigma^{2}} \sum_{i=1}^{n}\left(X_{i}-\mu\right)=0 \\
\frac{\partial \log L}{\partial\left(\sigma^{2}\right)}=-\frac{n}{2 \sigma^{2}}+\frac{1}{2 \sigma^{4}} \sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}=0
\end{gathered}
$$
由第一式得出 $\mu$ 的解为
$$
\mu^{*}=\sum_{i=1}^{n} X_{i} / n=\bar{X}
$$
以此代人第二式的 $\mu$, 得到 $\sigma^{2}$ 的解为
$$
\sigma^{* 2}=\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2} / n=m_{2}
$$
# unbiased estimator
实际上这里参数类似于一个待定的系数，等着我们去求（右边是概率论，左边是数理统计）

Definition
An estimator $\hat{\theta}$ is an unbiased estimator for $\theta$ if $E(\hat{\theta})=\theta$ (for all $\theta$ ).
$\widehat{\theta}$ is (weakly) consistent if $\hat{\theta} \rightarrow \theta$ weakly, that is $\lim _{n \rightarrow \infty} P(|\hat{\theta}-\theta|<c)=1 \forall c>0$.
我们实际上可以理解为收敛到一个最佳估计的点

Definition. The mean squared error (MSE) of an estimator $\hat{\theta}$ of $\theta$ is $E\left[(\hat{\theta}-\theta)^{2}\right]$.
One property of MSE. $\operatorname{MSE}(\widehat{\theta})=\operatorname{Var}(\hat{\theta})+$ bias $^{2}(\hat{\theta})$ where the bias of an estimator is $E[\hat{\theta}]-\theta$
我想知道总体的g(theta)（这里的g就是你现在非常想要知道的一个量，可以使平均数，也可以是方差，也可以是平均数/方差，但是这个量是由$\theta$运算出来的）是啥，但是呢我手上只有x_1,x_2,x_3....这些样本，那我怎么用估计出来呢，我们就用$\hat g$（注意的是，这边的$\hat g$和g不是同一个函数，因为他们的定义域就不一样了），然后我用这些样本x_1,x_2扔到$\hat g(x)$里面反应一下，希望他能够生成一个数来估计这个这个g(theta)(这也是前面的矩阵的 Method of moments estimator和maximum likehood的想法)但是呢，样本是可能bias的，这就导致算出来的值可能跟真实值差距比较大，这时候我们希望把所有$\hat g$的可能值都弄出来取个期望，这样的话结果会显得比较“中庸”一些

实际上unbias是一个测试（检验方法）方法，看得到的结论是否是有偏的

在样本方差里面之所以n-1，是因为

为证明这一点, 以 $a$ 记总体分布均值: $E\left(X_{i}\right)=a$. 也有 $E(\bar{X})=a$, 把 $X_{i}-\bar{X}$ 写为 $\left(X_{i}-a\right)-(\bar{X}-a)$, （如果a确定了的话，那么这边就不是$\bar{X}$而是a了，有
$$
\begin{aligned}
&\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\sum_{i=1}^{n}\left[\left(X_{i}-a\right)-(\bar{X}-a)\right]^{2} \\
&\quad=\sum_{i=1}^{n}\left(X_{i}-a\right)^{2}-2(\bar{X}-a) \sum_{i=1}^{n}\left(X_{i}-a\right)+n(\bar{X}-a)^{2}
\end{aligned}
$$
注意到 $\sum_{i=1}^{n}\left(X_{i}-a\right)=n(\bar{X}-a)$, 有
$$
\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\sum_{i=1}^{n}\left(X_{i}-a\right)^{2}-n(\bar{X}-a)^{2}
$$
因 $a=E\left(X_{i}\right)=E(\bar{X})$, 有（在这一步，如果a是确定了的话，那么这里等式不成立）
$$
\begin{gathered}
E\left(X_{i}-a\right)^{2}=\operatorname{Var}\left(X_{i}\right)=\sigma^{2}, i=1, \cdots, n \\
E(\bar{X}-a)^{2}=\operatorname{Var}(\bar{X})=\sum_{i=1}^{n} \operatorname{Var}\left(X_{i}\right) / n^{2}=n \sigma^{2} / n^{2}=\sigma^{2} / n
\end{gathered}
$$
于是得到
$$
E\left(S^{2}\right)=\frac{1}{n-1} E\left(\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\right)=\frac{1}{n-1}\left(n \sigma^{2}-n \cdot \sigma^{2} / n\right)=\sigma^{2}
$$
这就说明了 $S^{2}$ 是 $\sigma^{2}$ 的无偏估计.

# standard error

For a point estimator $\widehat{\Theta}$, the standard error of $\widehat{\Theta}$ is defined as the square root of its variance, i.e.
$$
S E_{\widehat{\Theta}}=\sqrt{\operatorname{Var}(\widehat{\Theta})}
$$
这里想说的就是点估计虽然最终结果是收敛的，但是他的方差还是存在的，就会存在一些误差，也就是error，所以说我们接下来引入区间估计

# confidence interval

定义 4.1 如果不论参数 $\theta$ 在参数空间 $\Theta$ 中取什么值, “区间 $\left[\theta_{1}(X)\right.$, $\left.\theta_{2}(X)\right]$ 包含 $\theta$ '这个事件的概率总不小于指定的常数 $1-\alpha(0 \leqslant \alpha \leqslant 1, \alpha$ 通常 很小), 即
$$
P_{\theta}\left(\theta_{1}(X) \leqslant \theta \leqslant \theta_{2}(X)\right) \geqslant 1-\alpha \quad(\text { 切 } \theta \in \Theta),
$$
则称 $\left[\theta_{1}(X), \theta_{2}(X)\right]$ 有置信水平 $1-\alpha$. 也常称 $\left[\theta_{1}(X), \theta_{2}(X)\right]$ 是 $\theta$ 的置信水 平 $1-\alpha$ 的区间估计或置信区间.